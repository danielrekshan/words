#!/usr/bin/env ruby

# std includes
require 'pathname'

# gem includes
require 'rubygems'

# standard library includes
#require 'trollop'
require 'zlib'
require 'net/http'
require 'optparse'
require 'pp'

# local includes
require File.join(File.dirname(__FILE__), '..', 'lib', 'words.rb')

POS_FILE_TYPES = %w{ adj adv noun verb }
POS_FILE_TYPE_TO_SHORT = { 'adj' => 'a', 'adv' => 'r', 'noun' => 'n', 'verb' => 'v' }

puts "Words Dataset Constructor 2010 (c) Roja Buck"

opts = { :quiet => false, :build_tokyo => false, :build_tokyo_with_evocations => false, :build_pure_evocations => false, :wordnet => 'Search...' }

optparse = OptionParser.new do|option|

    option.on( '-q', '--quiet', "Don't output verbose program detail. (Default: false)" ) do
	opts[:quiet] = true
    end

    option.on( '-w', '--wordnet FILE', "Location of the wordnet dictionary directory. (Default: Search)" ) do|f|
	opts[:wordnet] = f
    end

    option.on( '-t', '--build-tokyo', "Build the tokyo wordnet dataset? (Default: false)" ) do
	opts[:build_tokyo] = true
    end

    option.on( '-x', '--build-tokyo-with-evocations', "Build the tokyo dataset with the similarity dataset based on the wordnet evocation project? (Default: false) NOTE: requires internet connection." ) do
	opts[:build_tokyo_with_evocations] = true
    end

    option.on( '-e', '--build-pure-evocations', "Build the similarity dataset based on the wordnet evocation project for use with the pure words mode. (Default: false) NOTE: requires internet connection." ) do
	opts[:build_pure_evocations] = true
    end

    option.on( '-h', '--help', 'Display this screen' ) do
	puts option
	exit
    end

end

optparse.parse!

if !opts[:build_tokyo] && !opts[:build_tokyo_with_evocations] && !opts[:build_pure_evocations]
    puts "ERROR: You need to specify at least one dataset you want to build."
    exit
end
puts "Verbose mode enabled" if (VERBOSE = !opts[:quiet])

pp "Options:", opts

exit


require 'rufus-tokyo' if opts[:build_tokyo] || opts[:build_tokyo_with_evocations]

gem_path = Pathname.new "#{File.dirname(__FILE__)}/.."
abort "Ensure you run the command using sudo or as a Superuser / Administrator" unless gem_path.writable?
data_path = gem_path + "data/"
data_path.mkpath

wordnet_dir = nil
if opts[:wordnet] == "Search..."
    wordnet_dir = Words::Wordnet.locate_wordnet :search
    abort( "Unable to locate wordnet dictionary. To specify check --help." ) if wordnet_dir.nil?
else
    wordnet_dir = Words::Wordnet.locate_wordnet opts[:wordnet]
    abort( "Unable to locate wordnet dictionary in directory #{opts[:wordnet]}. Please check and try again." ) if wordnet_dir.nil?
end

# At this point we know we should have a wordnet directory within wordnet_dir
puts "Found wordnet files in #{wordnet_dir}..." if VERBOSE

index_files = POS_FILE_TYPES.map { |pos| wordnet_dir + "index.#{pos}" }
data_files = POS_FILE_TYPES.map { |pos| wordnet_dir + "data.#{pos}" }

(index_files + data_files).each do |required_file|
    abort( "Unable to locate #{required_file} within the wordnet dictionary. Please check your wordnet copy is valid and try again." ) unless required_file.exist?
    abort( "Cannot get readable permissions to #{required_file} within the wordnet dictionary. Please check the file permissions and try again." ) unless required_file.readable?
end

# At this point we know we have the correct files, though we don't know there validity
puts "Validated existance of wordnet files in #{wordnet_dir}..." if VERBOSE

# Build data

index_hash = Hash.new
data_hash = Hash.new
POS_FILE_TYPES.each do |file_pos|
  
    puts "Building #{file_pos} indexes..." if VERBOSE
  
    # add indexes
    (wordnet_dir + "index.#{file_pos}").each_line do |index_line|
	next if index_line[0, 2] == "  "
	index_parts = index_line.split(" ")
    
	lemma, pos, synset_count, pointer_count = index_parts.shift, index_parts.shift, index_parts.shift.to_i, index_parts.shift.to_i
	pointer_symbols = Array.new(pointer_count).map { POS_FILE_TYPE_TO_SHORT[file_pos] + index_parts.shift }
	sense_count = index_parts.shift
	tagsense_count = pos + index_parts.shift
	synset_ids = Array.new(synset_count).map { POS_FILE_TYPE_TO_SHORT[file_pos] + index_parts.shift }
    
	index_hash[lemma] = { "synset_ids" => [], "tagsense_counts" => [] } if index_hash[lemma].nil?
	index_hash[lemma] = { "lemma" => lemma, "synset_ids" => index_hash[lemma]["synset_ids"] + synset_ids, "tagsense_counts" => index_hash[lemma]["tagsense_counts"] + [tagsense_count] }
    
    end
  
    if opts[:build_tokyo] || opts[:build_tokyo_with_evocations]
	puts "Building #{file_pos} data..." if VERBOSE
    
	# add data
	(wordnet_dir + "data.#{file_pos}").each_line do |data_line|
	    next if data_line[0, 2] == "  "
	    data_line, gloss = data_line.split(" | ")
	    data_parts = data_line.split(" ")
      
	    synset_id, lexical_filenum, synset_type, word_count = POS_FILE_TYPE_TO_SHORT[file_pos] + data_parts.shift, data_parts.shift, data_parts.shift, data_parts.shift.to_i(16)
	    words = Array.new(word_count).map { "#{data_parts.shift}.#{data_parts.shift}" }
	    relations = Array.new(data_parts.shift.to_i).map { "#{data_parts.shift}.#{data_parts.shift}.#{data_parts.shift}.#{data_parts.shift}" }
      
	    data_hash[synset_id] = { "synset_id" => synset_id, "lexical_filenum" => lexical_filenum, "synset_type" => synset_type,
		"words" => words.join('|'), "relations" => relations.join('|'), "gloss" => gloss.strip }
	end
    end
  
end

score_hash = Hash.new
if opts[:build_tokyo_with_evocations] || opts[:build_pure_evocations]
    puts "Downloading score data..." if VERBOSE
    scores_file = data_path + "scores.txt.gz"
    scores_file.delete if scores_file.exist?
    File.open(scores_file,'w') do |file|
	file.write Net::HTTP.get(URI.parse('http://cloud.github.com/downloads/roja/words/scores.txt.gz'))
    end
    abort( "Unable to gather similarities information from http://cloud.github.com/downloads/roja/words/scores.txt.gz... Try again later." ) unless scores_file.exist?
  
    puts "Compiling score data..." if VERBOSE
    Zlib::GzipReader.open(scores_file) do |gz|
	gz.each_line do |line|
	    mean, median, sense1, sense2 = line.split(',')
	    senses = [sense1, sense2].map! { |sense| sense.strip.split('.') }.map! { |sense| index_hash[sense[0]]["synset_ids"].select { |synset_id| synset_id[0,1] == sense[1].gsub("s", "a") }[sense[2].to_i-1] }
	    senses.each do |sense|
		relation = (senses - [sense]).first.nil? ? sense : (senses - [sense]).first
		score_name = sense + "s"
		score_hash[score_name] = { "relations" => [], "means" => [], "medians" => [] } if score_hash[score_name].nil?
		score_hash[score_name] = { "relations" => score_hash[score_name]["relations"] << relation, "means" => score_hash[score_name]["means"] << mean, "medians" => score_hash[score_name]["medians"] << median }
	    end unless senses.include? nil
	end
    end
end

if opts[:build_tokyo] || opts[:build_tokyo_with_evocations]
    tokyo_hash = Rufus::Tokyo::Table.new((data_path + "wordnet.tct").to_s)
    index_hash.each { |k,v| tokyo_hash[k] = { "lemma" => v["lemma"], "synset_ids" => v["synset_ids"].join('|'), "tagsense_counts" => v["tagsense_counts"].join('|') } }
    data_hash.each { |k,v| tokyo_hash[k] = v }
    score_hash.each { |k,v| tokyo_hash[k] = { "relations" => v["relations"].join('|'), "means" => v["means"].join('|'), "medians" => v["medians"].join('|') } } if opts[:build_tokyo_with_evocations]
    tokyo_hash.close
end

if opts[:build_pure_evocations]
    score = Hash.new
    score_hash.each { |k,v| score[k] = [v["relations"].join('|'), v["means"].join('|'), v["medians"].join('|')] }
    File.open(data_path + "evocations.dmp",'w') do |file|
	file.write Marshal.dump(score)
    end
end

